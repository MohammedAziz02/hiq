"""
+ðŸ”µ
â”œâ”€â”€ +embeddings (BertEmbeddings)
â”‚   â”œâ”€â”€ +word_embeddings (Embedding) weight:[30522, 768]
â”‚   â”œâ”€â”€ +position_embeddings (Embedding) weight:[512, 768]
â”‚   â”œâ”€â”€ +token_type_embeddings (Embedding) weight:[2, 768]
â”‚   â””â”€â”€ +LayerNorm (LayerNorm) weight:[768] bias:[768]
â”œâ”€â”€ +encoder (BertEncoder)
â”‚   â””â”€â”€ +layer (ModuleList)
â”‚       â””â”€â”€ 0-11(BertLayer)
â”‚           â”œâ”€â”€ +attention (BertAttention)
â”‚           â”‚   â”œâ”€â”€ +self (BertSelfAttention)
â”‚           â”‚   â”‚   â””â”€â”€ query,key,value(Linear) weight:[768, 768] bias:[768]
â”‚           â”‚   â””â”€â”€ +output (BertSelfOutput)
â”‚           â”‚       â”œâ”€â”€ +dense (Linear) weight:[768, 768] bias:[768]
â”‚           â”‚       â””â”€â”€ +LayerNorm (LayerNorm) weight:[768] bias:[768]
â”‚           â”œâ”€â”€ +intermediate (BertIntermediate)
â”‚           â”‚   â””â”€â”€ +dense (Linear) weight:[3072, 768] bias:[3072]
â”‚           â””â”€â”€ +output (BertOutput)
â”‚               â”œâ”€â”€ +dense (Linear) weight:[768, 3072] bias:[768]
â”‚               â””â”€â”€ +LayerNorm (LayerNorm) weight:[768] bias:[768]
â””â”€â”€ +pooler (BertPooler)
    â””â”€â”€ +dense (Linear) weight:[768, 768] bias:[768]
"""

from transformers import BertModel
from hiq.vis import print_model

model = BertModel.from_pretrained("bert-base-uncased")

print("*" * 80)
print_model(model)
