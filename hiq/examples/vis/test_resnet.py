"""
+ðŸ”µ
â”œâ”€â”€ +conv1 (Conv2d) weight:[64, 3, 7, 7]
â”œâ”€â”€ +bn1 (BatchNorm2d) weight:[64] bias:[64]
â”œâ”€â”€ +layer1 (Sequential)
â”‚   â”œâ”€â”€ +0 (Bottleneck)
â”‚   â”‚   â”œâ”€â”€ +conv1 (Conv2d) weight:[64, 64, 1, 1]
â”‚   â”‚   â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[64] bias:[64]
â”‚   â”‚   â”œâ”€â”€ +conv2 (Conv2d) weight:[64, 64, 3, 3]
â”‚   â”‚   â”œâ”€â”€ +conv3 (Conv2d) weight:[256, 64, 1, 1]
â”‚   â”‚   â”œâ”€â”€ +bn3 (BatchNorm2d) weight:[256] bias:[256]
â”‚   â”‚   â””â”€â”€ +downsample (Sequential)
â”‚   â”‚       â”œâ”€â”€ +0 (Conv2d) weight:[256, 64, 1, 1]
â”‚   â”‚       â””â”€â”€ +1 (BatchNorm2d) weight:[256] bias:[256]
â”‚   â””â”€â”€ 1-2(Bottleneck)
â”‚       â”œâ”€â”€ +conv1 (Conv2d) weight:[64, 256, 1, 1]
â”‚       â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[64] bias:[64]
â”‚       â”œâ”€â”€ +conv2 (Conv2d) weight:[64, 64, 3, 3]
â”‚       â”œâ”€â”€ +conv3 (Conv2d) weight:[256, 64, 1, 1]
â”‚       â””â”€â”€ +bn3 (BatchNorm2d) weight:[256] bias:[256]
â”œâ”€â”€ +layer2 (Sequential)
â”‚   â”œâ”€â”€ +0 (Bottleneck)
â”‚   â”‚   â”œâ”€â”€ +conv1 (Conv2d) weight:[128, 256, 1, 1]
â”‚   â”‚   â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[128] bias:[128]
â”‚   â”‚   â”œâ”€â”€ +conv2 (Conv2d) weight:[128, 128, 3, 3]
â”‚   â”‚   â”œâ”€â”€ +conv3 (Conv2d) weight:[512, 128, 1, 1]
â”‚   â”‚   â”œâ”€â”€ +bn3 (BatchNorm2d) weight:[512] bias:[512]
â”‚   â”‚   â””â”€â”€ +downsample (Sequential)
â”‚   â”‚       â”œâ”€â”€ +0 (Conv2d) weight:[512, 256, 1, 1]
â”‚   â”‚       â””â”€â”€ +1 (BatchNorm2d) weight:[512] bias:[512]
â”‚   â””â”€â”€ 1-7(Bottleneck)
â”‚       â”œâ”€â”€ +conv1 (Conv2d) weight:[128, 512, 1, 1]
â”‚       â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[128] bias:[128]
â”‚       â”œâ”€â”€ +conv2 (Conv2d) weight:[128, 128, 3, 3]
â”‚       â”œâ”€â”€ +conv3 (Conv2d) weight:[512, 128, 1, 1]
â”‚       â””â”€â”€ +bn3 (BatchNorm2d) weight:[512] bias:[512]
â”œâ”€â”€ +layer3 (Sequential)
â”‚   â”œâ”€â”€ +0 (Bottleneck)
â”‚   â”‚   â”œâ”€â”€ +conv1 (Conv2d) weight:[256, 512, 1, 1]
â”‚   â”‚   â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[256] bias:[256]
â”‚   â”‚   â”œâ”€â”€ +conv2 (Conv2d) weight:[256, 256, 3, 3]
â”‚   â”‚   â”œâ”€â”€ +conv3 (Conv2d) weight:[1024, 256, 1, 1]
â”‚   â”‚   â”œâ”€â”€ +bn3 (BatchNorm2d) weight:[1024] bias:[1024]
â”‚   â”‚   â””â”€â”€ +downsample (Sequential)
â”‚   â”‚       â”œâ”€â”€ +0 (Conv2d) weight:[1024, 512, 1, 1]
â”‚   â”‚       â””â”€â”€ +1 (BatchNorm2d) weight:[1024] bias:[1024]
â”‚   â””â”€â”€ 1-35(Bottleneck)
â”‚       â”œâ”€â”€ +conv1 (Conv2d) weight:[256, 1024, 1, 1]
â”‚       â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[256] bias:[256]
â”‚       â”œâ”€â”€ +conv2 (Conv2d) weight:[256, 256, 3, 3]
â”‚       â”œâ”€â”€ +conv3 (Conv2d) weight:[1024, 256, 1, 1]
â”‚       â””â”€â”€ +bn3 (BatchNorm2d) weight:[1024] bias:[1024]
â”œâ”€â”€ +layer4 (Sequential)
â”‚   â”œâ”€â”€ +0 (Bottleneck)
â”‚   â”‚   â”œâ”€â”€ +conv1 (Conv2d) weight:[512, 1024, 1, 1]
â”‚   â”‚   â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[512] bias:[512]
â”‚   â”‚   â”œâ”€â”€ +conv2 (Conv2d) weight:[512, 512, 3, 3]
â”‚   â”‚   â”œâ”€â”€ +conv3 (Conv2d) weight:[2048, 512, 1, 1]
â”‚   â”‚   â”œâ”€â”€ +bn3 (BatchNorm2d) weight:[2048] bias:[2048]
â”‚   â”‚   â””â”€â”€ +downsample (Sequential)
â”‚   â”‚       â”œâ”€â”€ +0 (Conv2d) weight:[2048, 1024, 1, 1]
â”‚   â”‚       â””â”€â”€ +1 (BatchNorm2d) weight:[2048] bias:[2048]
â”‚   â””â”€â”€ 1-2(Bottleneck)
â”‚       â”œâ”€â”€ +conv1 (Conv2d) weight:[512, 2048, 1, 1]
â”‚       â”œâ”€â”€ bn1,bn2(BatchNorm2d) weight:[512] bias:[512]
â”‚       â”œâ”€â”€ +conv2 (Conv2d) weight:[512, 512, 3, 3]
â”‚       â”œâ”€â”€ +conv3 (Conv2d) weight:[2048, 512, 1, 1]
â”‚       â””â”€â”€ +bn3 (BatchNorm2d) weight:[2048] bias:[2048]
â””â”€â”€ +fc (Linear) weight:[1000, 2048] bias:[1000]
"""
import torch
import torchvision
from hiq.vis import print_model


model = torchvision.models.resnet152()
model.conv1.weight.requires_grad = False
model.layer3[1].conv1.weight.requires_grad = False
model.layer4[0].bn3.bias.requires_grad = False
model = model.cuda()
with torch.no_grad():
    print(model.layer3[2].conv1.weight.requires_grad)
    print("*" * 80)
    print_model(model)
    #print_model(model, max_depth=3)
    #print_model(model, only_types=("Conv2d","Linear",'Bottleneck','Sequential'))
    #print_model(model, only_names=('layer1','0','bn3','conv1'))
